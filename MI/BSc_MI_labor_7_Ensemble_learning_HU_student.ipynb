{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrMrtn/GoogleColab/blob/main/MI/BSc_MI_labor_7_Ensemble_learning_HU_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![flexsys_logo.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFoAAABFCAYAAADKKPFMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAGOAAABjgBco5mFwAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAAUySURBVHic7ZxNiBxFGIbfr2dMgpOkN+q6xhBD1MN2dA2J6MGLXgRPXtRcRcL6gyBGFA9hd34i6EEQBUFQEAQPUdCIgkJWDx4EUdlFyQwxEhHMZvPLzmZ3NrLpej2soJmuqrF7Zmp7l3qO885X9fXbM1Vf/1QJPFdRC3ftI3hYpwkwMd5sPJil3aC7tNYeBMWsgVnb9UY7whvtCG+0I7zRjihmDaxt3vUZhdtShAwAYpxoAEAKeHj8Yv1Y1pzyTGajKWoEkJ0po6yqirE+az55xw8djvBGO8Ib7QhvtCNyZXQRKlf59JLMVQfsJcQJAJcBKbULArWDkIIuKA4Kg13kk4CAVAFtSVkBVC/76kQ3Rttq4mfLzcZRnVANo9MAbtI2SMx0kU+CWhj9IMDdWk1k7/hsfbKX/dlYs3/VvOGNdoQ32hHeaEd4ox3RTdXRc0h8WA2jVqoYYLLSbIxqRZG3hdiqkwpLwXSGFO+qhtFHOmFT89LjL+DPRVNgrowGGKWNEGDBpJVn6+93l0+CIQCP6YSlG0r7cd4cuBaGjjtXOoH/w6o3WgDtVWbeWPVGrxa80Y7I2WTYW2pbhkegsE6nqVKpUZn+KVWF0w25MpqQZwD+ppEeEOCgPgYnje0p+RzADq3YurwHwFTKFH8G5BWdsHh+6yJw3BiYK6MDwffjs43EHbVqGF1vCbvUx5TaOVNu1j/WS3VroB+jHeGNdoQ32hHeaEd4ox3hjXaEN9oR3mhHeKMd4Y12hDfaEd5oR3ijHeGNdoQ32hHeaEd4ox3hjXbESjzKMr7AriCGVQTqFtNvQoCd1TB6TacR2GLqTMjnqmF0NhFDRmJadypIs4D1KpwbXW42tG/7W2GwzXR6CGwH8LJOsy7TBZ/QxtgW9xJD1iYtODf69VvvmyisX3dj++dq6cpS69y5R8eax393nZMLnBtdGhq8/envjiReAfji+crZyQ8Ob3SdTxrYxRN3PxmmIAB+7SI2H0iHnQ9WO7kxGgIoy35Gq51cvalkIpBr3iT/+lGnSSAtEn+4yEMoc1lji7VwWLvgEQBUXFiszB+zv+vkglhtYCHQ17DEhfFm4xOddOi64VEV6xZ0igqIN8bmGifalUoY7RXwSW1fBZnC8qrgBLWBO3aT6iHDEagiIV/DcKEgBV4EcJshuKeICAIWtUMHg3g3IIMEv9LIYwDe08WpGKOA3KORWkrwKTSmFYUjMeUpbR6KUwDe0WqM74VIicS37ZoArxZFsEDyZl0wgMSV00ohgnp5tjHR/nktjF5K3xji7DvYWaD8UmnWtTm6nwzJ1IdI6a0tthlXKdNtgE5tmuMISH6qjr5gO3h9hWPfrTF7VVSkgu0UmxXKEQbQb/twBadTZyICZemP7F3pZ9sW00anIDF8RQBm3yUs4FugYSOTzZtmMJ+1ZTcIO/qWgJ29NpLZaKEcJYrfJD4HRzC/eAjAl+kaNB+EQMi+zF6964sdxhWr0bZAAU6Wm/VEGVQdiA5k8UREGFhyNf0tO2DIRMzSskr9r7fDjVdL/j2fDKWLrX9dQhSynLjMQ0dfqg7rHszm6q4v+zanJWtfIvaysOdGZzZFBMpSWWSsFEy5WNvKFAT7BNuXX7RtPLU+KnKIZDhx3VUdATcK5ZShYeOJIDBcC6PETmAktpN80RQ3Nz2z4d37HznDWMX8zzgyd2qmZJoMCaFADtTCaF97c5Tklm//Igsgp6X9ypK41nRLVqAIyCyA1vJXsQTwn63b5IKpJwKE4GAtjPZrtD1/Ay3aiOaEeIwPAAAAAElFTkSuQmCC) Mesterséges Intelligencia és Rendszertervezés Tanszék, ©2024. BME-MIT, Vetró Mihály, Dr. Hullám Gábor  \n",
        "# **VIMIAC16 - Mesterséges Intelligencia**\n",
        "## 2024. Őszi félév\n",
        "## **7. Laborgyakorlat**\n"
      ],
      "metadata": {
        "id": "9zC0DJG9dEW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Együttes tanulás**"
      ],
      "metadata": {
        "id": "Gy0skNGnD4UU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfpIun-GBD-J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Motiváció**\n",
        "\n",
        "Felügyelt tanulásnál egy konkrét feladat megoldása során az esetek túlnyomó többségében *egyetlen prediktív modellt* alkalmazunk, amely modell a probléma jellegétől (pl. mit és mi alapján szeretnénk prediktálni) és gyakorlati aspektusoktól (pl. mekkora mennyiségű és mennyire zajos adat áll rendelkezésünkre) függően lehet döntési fa, logisztikus regresszió, neurális háló, stb...\n",
        "\n",
        "Felmerülhet a kérdés azonban, hogy lehetséges-e az előrejelzés pontosságát és megbízhatóságát javítani azáltal, hogy egyetlen modell helyett *több, különböző modell* predikciójának együttesét (pl. átlagát vagy egyéb aggregált eredményét) használjuk fel. Azt a folyamatot, amely során ugyanazon (vagy nagyon hasonló) probléma megoldására több, különböző modellt készítünk, majd ezen modellek predikcióját aggregáljuk a végleges kimenet előállításához, **együttes tanulásnak** nevezzük.\n",
        "\n",
        "Az együttes tanulás hátrányait könnyű belátni, legfőképpen abból kiindulva, hogy egyetlen modell helyett egy több elemből álló modellhalmazt kell létrehoznunk, amely megnövekedett erőforrásigénnyel jár. Más szempontból azonban az együttes tanulás eredményéül előálló modellhalmaz aggregált prediktív teljesítménye rendszerint felülmúlja az azt alkotó modellek egyéni teljesítményét és általánosítóképességét egyaránt.\n",
        "\n",
        "> Utóbbi tényt könnyű belátni egy valós példán keresztül: mint ahogyan több empirikus eredmény is bizonyította, egyes emberek becslési képessége az egyéni torzítás (vagy szakszóval élve: \"bias\") okán rendszerint pontatlan. Ez nyilvánvalóvá válik akkor, ha például megkérünk valakit, hogy becsülje meg pusztán ránézésre, hogy hány darab üveggolyó van egy áttetsző edényben, ekkor ugyanis a becslések többsége jelentősen eltérő lesz a valós számtól. Viszont, hogyha megfelelően sok ember becslését kiátlagoljuk, az így keletkező eredmény erősen közelíteni fogja a valós számot. Ennek legfőbb oka az, hogy az egyes becslések hibája (feltehetően) egymástól független, az ilyen jellegű hiba pedig több kísérlet eredményének aggregálásával csökkenthető."
      ],
      "metadata": {
        "id": "vN6xPLAHD8yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Bagging és véletlen erdők**\n",
        "\n",
        "Az együttes tanulás megvalósításának egyik legalapvetőbb módja a **Bagging**, amely — kissé leegyszerűsítve — az alábbi lépésekből áll:\n",
        "\n",
        "1. Vegyünk kiindulásul egy $N$ rekordból és $K$ változóból álló adathalmazt.\n",
        "2. Válasszuk ki egyenletes eloszlás szerint a változók $k \\in K$ valódi részhalmazát (ezek a $K*N$ méretű adatmátrix oszlopai), majd visszahelyezéses mintavételezéssel (**Bootstrapping**) egy $N$ méretű rekordhalmazt a kiválasztott változókra.\n",
        "3. Illesszünk egy modellt az így kapott adathalmazra.\n",
        "4. Ismételjük a **2.** és **3.** lépéseket, amíg egy előre meghatározott számú modellt nem kapunk.\n",
        "5. (**Aggregation**) Az így kapott modellhalmaz új mintára adott predikcióját osztályozás esetén többségi döntéssel, regresszió esetén átlagolással állapítjuk meg.\n",
        "\n",
        "> Vegyük észre, hogy a **2.** lépésben (**Bootstrapping**) mintavételezéssel kialakított adathalmazban a változók és minták csak egy részhalmaza szerepel, illetve egy minta többször is szerepelhet a visszahelyezéses mintavételezés miatt. Ez a lépés különösen fontos, mivel így biztosítható, hogy a determinisztikus (pl. döntési fa) és alacsony varianciájú (pl. logisztikus regresszió) tanítási folyamattal rendelkező modelltípusok esetén is heterogén modellegyüttest hozzon létre az eljárás, ugyanis minden modell az eredeti adathalmaznak különböző részhalmazain tanul.\n",
        "\n",
        ">Megjegyzés: az eredeti Bagging algoritmus mindig a teljes változóhalmazra végezte a Bootstrap mintavételezést. Véletlen erdők esetén viszont előnyös, ha a változók halmaza is véletlenszerűen sorsolt, mivel ekkor az egyes modellek kevésbé korreláltak egymással. Ennek a technikának az általános neve *Feature Bagging*, véletlen erdőknél a Bagging részét képzi.\n",
        "\n",
        "Az eljárás sematikus folyamatábrája egy leegyszerűsített, 3 modellt létrehozó esetben az alábbi:\n",
        "\n",
        "<!-- ![bagging_algorithm](https://share.mit.bme.hu/index.php/s/RenmdjWN8LtXNfA/download/bagging_algorithm.png) -->\n",
        "\n",
        "<img src=\"https://share.mit.bme.hu/index.php/s/RenmdjWN8LtXNfA/download/bagging_algorithm.png\" alt=\"drawing\" width=\"900\"/>\n",
        "\n",
        "A Bagging algoritmus osztályozásra használt változatának egy lehetséges implementációja alább látható.\n",
        "\n",
        "**Tanulmányozza a kódot, majd futtassa le a kódblokkot, ezzel definiálva az osztályt!**"
      ],
      "metadata": {
        "id": "swI0MARWXrGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, clone\n",
        "from sklearn.utils import resample\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.stats import mode\n",
        "\n",
        "class BaggingClassifier:\n",
        "    \"\"\"\n",
        "    An implementation of the Bagging algorithm for classification tasks\n",
        "    for an arbitrary base estimator.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_estimator : object\n",
        "        A base estimator object implementing the `fit` and `predict` methods.\n",
        "        If None, then the base estimator is a decision tree.\n",
        "        Default: DecisionTreeClassifier()\n",
        "\n",
        "    n_estimators : int\n",
        "        The number of base estimators in the ensemble.\n",
        "        Default: 100\n",
        "\n",
        "    max_features : int or None\n",
        "        The number of features to consider when looking for the best split.\n",
        "        If None, all features are considered.\n",
        "        Default: None\n",
        "\n",
        "    random_state : int or None\n",
        "        The seed for the random number generator.\n",
        "        If None, the random number generator is not seeded.\n",
        "        Default: None\n",
        "    \"\"\"\n",
        "    def __init__(self, base_estimator=DecisionTreeClassifier(), n_estimators=100, max_features=None, random_state=None):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.estimators_ = []\n",
        "        self.features_ = []\n",
        "\n",
        "    @staticmethod\n",
        "    def bootstrap_samples_(X, y, max_features=None, random_state=None):\n",
        "        \"\"\"\n",
        "        Bootstrap samples from the input data.\n",
        "        Samples are drawn with replacement.\n",
        "        If max_features is specified, only a random subset of features is considered.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input data.\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The target values.\n",
        "\n",
        "        max_features : int or None\n",
        "            The number of features to consider when looking for the best split.\n",
        "            If None, all features are considered.\n",
        "            Default: None\n",
        "\n",
        "        random_state : int or None\n",
        "            The seed for the random number generator.\n",
        "            If None, the random number generator is not seeded.\n",
        "            Default: None\n",
        "        \"\"\"\n",
        "        # Sample the dataset with replacement sampling\n",
        "        X_sampled, y_sampled = resample(X, y, replace=True, random_state=random_state)\n",
        "        # Subsample the features if the max number of features is specified\n",
        "        if max_features is not None and max_features < X.shape[1]:\n",
        "            if random_state is not None:\n",
        "                np.random.seed(random_state)\n",
        "            features = np.random.choice(X.shape[1], size=max_features, replace=False)\n",
        "            X_sampled = X_sampled[:, features]\n",
        "            # Return the feature indices along the subsampled data\n",
        "            return X_sampled, y_sampled, features\n",
        "        return X_sampled, y_sampled\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the input data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input data.\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The target values.\n",
        "        \"\"\"\n",
        "        self.estimators_, self.features_ = [], []\n",
        "\n",
        "        # Produce the specified number of estimators\n",
        "        for i in range(self.n_estimators):\n",
        "            # Initialize a local version of the random state, if specified.\n",
        "            # Otherwise the bootstrapped samples would be the same for every model when the random state is specified.\n",
        "            if self.random_state is not None:\n",
        "                local_random_state = self.random_state * (i + 1) * self.n_estimators\n",
        "            else:\n",
        "                local_random_state = None\n",
        "\n",
        "            if self.max_features is not None and self.max_features < X.shape[1]:\n",
        "                # Sample the dataset with replacement and the specified number of features\n",
        "                X_sampled, y_sampled, features = self.bootstrap_samples_(X, y, max_features=self.max_features,\n",
        "                                                                         random_state=local_random_state)\n",
        "                self.features_.append(features)\n",
        "            else:\n",
        "                # Sample the dataset with replacement if the number of features is not specified\n",
        "                X_sampled, y_sampled = self.bootstrap_samples_(X, y, random_state=local_random_state)\n",
        "\n",
        "            # Train a copy of the base estimator on the sampled data\n",
        "            estimator = clone(self.base_estimator).fit(X_sampled, y_sampled)\n",
        "            self.estimators_.append(estimator)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for the input data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input data.\n",
        "        \"\"\"\n",
        "        if self.max_features is not None and self.max_features < X.shape[1]:\n",
        "            # Predict on the feature subset used for each estimator\n",
        "            predictions = np.array([estimator.predict(X[:, features]) for estimator, features in zip(self.estimators_, self.features_)])\n",
        "        else:\n",
        "            # Predict on the entire feature set for each estimator\n",
        "            predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n",
        "        # Return the average predicted value for every input sample\n",
        "        return mode(predictions, axis=0)[0]"
      ],
      "metadata": {
        "id": "A8A9722-XAGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vegyük észre, hogy a fenti osztályban a bootstrapping lépéshez külön statikus függvény tartozik (`bootstrap_samples_`), az aggregációt pedig a `predict` függvény végzi azáltal, hogy visszaadja az egyes modellek predikcióinak a móduszát (`mode` függvény), amely a többségi szavazással ekvivalens.\n",
        "\n",
        "A módszer teszteléséhez ezt követően hozzunk létre egy mesterségesen kialakított adathalmazt, majd osszuk azt tanító- és teszthalmazra az alábbi kódblokk segítségével:"
      ],
      "metadata": {
        "id": "Eh6fnVJ51xh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a dummy classification problem\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=16, n_redundant=0,\n",
        "                           flip_y=0.01, n_clusters_per_class=2,\n",
        "                           hypercube=True, class_sep=1.,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "kgF_BNbLStOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Az így keletkező adathalmaz véletlenszerűen generált mintákból áll, amelyek adott számú, normál eloszlású klasztert alkotnak az n-dimenziós térben minden osztályhoz. További információval az `sklearn` csomag [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) függvényének dokumentációja szolál.\n",
        "\n",
        "A fenti beállítások mellett egy összesen 1000 mintából és 20 folytonos értékű változóból álló adathalmaz áll elő, amelyet egy 800 mintás tanító- és 200 mintás teszt részhalmazra osztottunk. Ezek méretét az alábbi módon tudjuk ellenőrízni:"
      ],
      "metadata": {
        "id": "7bMQKQfe4hMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "g0qkl38ghnCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ezt követően definiáljuk az alábbi függvényt, amely elvégzi egy előre definiált prediktor tanítását a generált adathalmazon, majd visszaadja a betanított modell teszthalmazon mért pontosságát:"
      ],
      "metadata": {
        "id": "cHvM8emt8_3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_classifier(classifier, runs=10):\n",
        "    accuracies_local = []\n",
        "    for _ in range(runs):\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracies_local.append(accuracy)\n",
        "    return np.mean(accuracies_local)"
      ],
      "metadata": {
        "id": "Hg-4gpUd7xoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mivel a tanítás folyamata (a bootstrapping miatt) nemdeterminisztikus, a fenti kiértékelő függvény többször megismétli azt, és a pontosságok átlagát adja vissza, így a kapott pontosságérték bizonytalansága alacsonyabb.\n",
        "\n",
        "Végül inicializáljuk a Bagging osztályozót az alábbi módon, és figyeljük meg, hogy milyen pontosságot ér el a mesterséges adathalmazon:"
      ],
      "metadata": {
        "id": "x4m3gZNt9kif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create a BaggingClassifier with Decision Trees as base estimators\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                  n_estimators=20,\n",
        "                                  max_features=int(np.sqrt(X.shape[1])))\n",
        "\n",
        "accuracy = evaluate_classifier(bagging_model, runs=10)\n",
        "print(f\"Bagging Classifier Accuracy (Decision Tree): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "l5xcnI9CSHSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fenti példában egy korlátozott maximális mélységű döntési fát (`base_estimator=DecisionTreeClassifier(max_depth=3)`) adtunk meg alapmodellként, amelyből 20 példányt (`n_estimators=20`) létrehozva alkottunk modellegyüttest. Ezen felül specifikáltuk azt is, hogy az egyes modellekhez tartozó, Bootstrapping eljárással kiválasztott minták a teljes változóhalmaz méretének négyzetgyökével azonos számú változót tartalmaznak (egészre kerekítve: `max_features=int(np.sqrt(X.shape[1]))`). Az alapmodell kiválasztása itt nem véletlen, ugyanis a széles körben ismert [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) algoritmus működése is a Bagging módszeren alapul, döntési fa alapmodellel. Fontos azonban, hogy a Bagging algoritmus paraméterei (az alapmodellt is beleértve) tetszőlegesen megválaszthatóak.\n",
        "\n",
        "**Az alábbi kódblokkban a fentihez hasonló módon definiáljon, majd értékeljen ki egy Bagging osztályozót, eltérő paraméterekkel!**\n",
        "\n",
        "1. **Figyelje meg a döntési fa mélységének, illetve a modellek számának hatását a pontosságra!**\n",
        "2. **Próbáljon ki egy eltérő osztályozót is alapmodellként!**\n",
        "\n",
        "> Tipp: alapmodellt érdemes az alábbi, `sklearn` csomagban definiált osztályozók közül választani:\n",
        "1. [Support-Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (RBF kernellel ajánlott)\n",
        "2. [Naive Bayes](https://scikit-learn.org/dev//modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
        "3. [K-Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "4. [Gaussian Process](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)"
      ],
      "metadata": {
        "id": "OWUWRlyX--iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO: create and evaluate a BaggingClassifier with different parameters!\n",
        "# bagging_model = ...\n",
        "################################################################################\n",
        "\n",
        "accuracy = evaluate_classifier(bagging_model, runs=10)\n",
        "print(f\"Bagging Classifier Accuracy (Decision Tree): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "RJYDVFhWSSNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO: create and evaluate a BaggingClassifier with a different baseline model!\n",
        "# bagging_model = ...\n",
        "################################################################################\n",
        "\n",
        "accuracy = evaluate_classifier(bagging_model, runs=10)\n",
        "print(f\"Bagging Classifier Accuracy (other baseline): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "kR5_wmVjHsRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Boosting**\n",
        "\n",
        "A Bagging algoritmus áttekintése során észrevehettük, hogy az egyes modellek egymástól teljesen független módon, véletlenszerűen kiválasztott minta alapján lettek tanítva. Ennek persze előnye, hogy az egyes modellek létrehozása párhuzamosítható, nagy hátránya viszont, hogy viszonylag nagy számú modellre van szükség ahhoz, hogy az aggregált predikció pontossága jelentősen jobb legyen az egyes modellekénél.\n",
        "\n",
        "Ez utóbbi problémára kínál megoldást a **Boosting** algoritmusok családja. Az ide tartozó módszerek szekvenciálisan adnak hozzá újabb modelleket a modellegyütteshez úgy, hogy minden hozzáadott modell valamilyen módon \"tanul\" a már elkészült modellek gyengeségeiből.\n",
        "\n",
        "Az egyik legkorábbi Boosting algoritmus az Adaptive Boosting (röviden **AdaBoost**), amelynek lényege, hogy minden modell tanítása során nagyobb súlyt kapnak azok a tanítópéldák, amelyekre a korábbi modellek hibás predikciót adtak, ezáltal az újonnan létrehozott modell azokon a mintákon jobban fog teljesíteni.\n",
        "\n",
        " Az AdaBoost módszer az alábbi lépésekből áll:\n",
        "1. Kiindulásként határozzunk meg uniform módon egy súlyértéket minden, az adathalmazban található rekordhoz.\n",
        "2. Tanítsunk be egy modellt a pillanatnyi súlyokkal.\n",
        "3. Növeljük azoknak a mintáknak a súlyát, amelyekre a legújabb modell hibás predikciót adott, majd normáljuk a módosított súlyvektort.\n",
        "4. Ismételjük a 2. lépéstől, amíg előre megadott számú modellt nem kapunk.\n",
        "5. A modellegyüttes új mintákra adott predikcióját úgy kapjuk, hogy az egyes modellek kimenetét súlyozott módon aggregáljuk. Az aggregáció során az egyes modellek súlya annál nagyobb, minél jobb prediktív teljesítményt adtak a tanítóhalmazon.\n",
        "\n",
        "> Az AdaBoost algoritmus (és általánosan a Boosting módszerek) iteratív módon javítják minden lépésben a meglévő modellhalmaz hibáit, felgyorsítva ezzel a folyamat konvergenciáját. Emiatt jellemzően hasonlóan jó prediktív teljesítmény eléréséhez kevesebb modell is elegendő ezen módszercsalád alkalmazásakor a Bagging algoritmushoz viszonyítva.\n",
        "\n",
        "Az AdaBoost algoritmus sematikus folyamatábrája 3 modell esetén az alábbi:\n",
        "<!-- ![adaboost_algorithm_simplified](https://share.mit.bme.hu/index.php/s/bW9FgHTr3xJpQdm/download/boosting_algorithm_simplified.png) -->\n",
        "\n",
        "<img src=\"https://share.mit.bme.hu/index.php/s/bW9FgHTr3xJpQdm/download/boosting_algorithm_simplified.png\" alt=\"drawing\" width=\"700\"/>\n",
        "\n",
        "Ahogy a fenti ábrán is látszik, minden modell tanítása az előző modell hibájától függ, emiatt az egyes modellek létrehozása nem párhuzamosítható, csak szekvenciális módon történhet.\n",
        "\n",
        "Az AdaBoost algoritmusnak több implementációja is létezik, mi a továbbiakban az `sklearn` csomagban foglalt változatot fogjuk használni. Gyakorlati szempontból az AdaBoost osztályozó példányosítása és használata a fent implementált Bagging módszerhez hasonló módon történik:"
      ],
      "metadata": {
        "id": "4iVJ9QCIRw96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "boosted_model_dt = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                      n_estimators=20,\n",
        "                                      algorithm=\"SAMME\")\n",
        "\n",
        "accuracy = evaluate_classifier(boosted_model_dt, runs=10)\n",
        "print(f\"Boosted Classifier Accuracy (Decision Tree): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "Pqdk1oDEpyqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Érdemes továbbá összehasonlítani, hogy az AdaBoost hogyan teljesít a Bagging algoritmussal szemben különböző méretű modellegyüttesek mellett. Ehhez futtassuk le a tanítást és a kiértékelést különböző modellszámokra, majd ábrázoljuk az eredményt az alábbi kódblokk segítségével:"
      ],
      "metadata": {
        "id": "-uieM-b91JIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "accuracy_values_bagging = []\n",
        "accuracy_values_boosting = []\n",
        "\n",
        "for n_estimators in range(1, 30, 1):\n",
        "    bagging_model_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                         n_estimators=n_estimators,\n",
        "                                         max_features=int(np.sqrt(X.shape[1])))\n",
        "    accuracy_values_bagging.append(evaluate_classifier(bagging_model_dt, runs=10))\n",
        "\n",
        "    boosted_model_dt = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                          n_estimators=n_estimators,\n",
        "                                          algorithm=\"SAMME\")\n",
        "    accuracy_values_boosting.append(evaluate_classifier(boosted_model_dt, runs=10))\n",
        "\n",
        "plt.plot(range(1, 30, 1), accuracy_values_bagging, label=\"Bagging Classifier (Decision Tree)\")\n",
        "plt.plot(range(1, 30, 1), accuracy_values_boosting, label=\"Boosted Classifier (Decision Tree)\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim((0.5, 1.))\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "VfyXO3zisYr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Meglátása szerint melyik módszer teljesített jobban azonos alapmodellek és modellegyüttes-méret mellett?**\n",
        "\n",
        "{válasz}\n",
        "\n",
        "***\n",
        "\n",
        "**Az eredmények alapján a két módszer prediktív teljesítménye milyen mértékben növekszik a modellek számával együtt? Lát különbséget a módszerek között ebben a tekintetben?**\n",
        "\n",
        "{válasz}\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "AHNZAfP91oWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Végül érdemes megemlíteni, hogy több, az AdaBoost-nál modernebb Boosting algoritmus is létezik, amelyek közül talán a gradiens alapú módszerek a legismertebbek. Ezekre jó példa az `sklearn`-ben implementált [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), illetve a külön csomagként létező [Extreme Gradient Boosting](https://xgboost.readthedocs.io/en/stable/)."
      ],
      "metadata": {
        "id": "DDkUX0Fs2bw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Stacking**\n",
        "\n",
        "Az eddigiekben alapvetésként feltételeztük, hogy a modellegyüttes minden tagja ugyanazon modellcsaládból (pl. döntésifa, SVM, stb...) kerül ki. Ez azonban nem feltétlen szükségszerű, egy modellegyüttes ugyanis tetszőleges típusú osztályozókból (vagy regresszorokból) is állhat egészen addig, amíg azok ugyanazon osztályozási (vagy regressziós) problémára kínálnak megoldást. Azt az általános esetet, amikor egy modellegyüttes eltérő modellcsaládból származó becslőkből tevődik össze, **Stacking**-nek nevezzük.\n",
        "\n",
        "A megvalósítás az eddigiekhez viszonyítva egyszerűbb: `sklearn` segítségével inicializálunk néhány egyéni osztályozót, majd azokból egy modellegyüttest ([`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)). Az így létrejövő becslőt egy modellként tanítjuk és értékeljük ki.\n",
        "\n",
        "Mindezt az alábbi kódblokk futtatásával tehetjük meg a már korábban legenerált mesterséges adathalmazon:"
      ],
      "metadata": {
        "id": "YFq0XowKSI2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "# Create a VotingClassifier with multiple base estimators\n",
        "stacking_model = VotingClassifier(estimators=[\n",
        "    ('decision_tree', DecisionTreeClassifier(max_depth=3)),\n",
        "    ('svm', SVC(kernel='rbf')),\n",
        "    ('naive_bayes', GaussianNB()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('gaussian_process', GaussianProcessClassifier())\n",
        "], voting='hard')\n",
        "\n",
        "accuracy = evaluate_classifier(stacking_model, runs=10)\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "D_MxkqgEwp1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A megvalósítás során 5 darab modellből állítottuk össze a modellegyüttest, amely modellek mindegyike drasztikusan eltérő módon oldja meg az osztályozási problémát. Erről részletesebb leírással az egyes modelleket megvalósító osztályok dokumentációja szolgál:\n",
        "\n",
        "1. [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "2. [Support-Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        "3. [Naive Bayes](https://scikit-learn.org/dev//modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
        "4. [K-Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "5. [Gaussian Process](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)\n",
        "\n",
        "> Érdemes megfigyelni, hogy a Stacking eljárás segítségével létrehozott, 5 eltérő modellt magába foglaló osztályozó lényegesen jobb teljesítményt ért el, mint a korábbi, egyetlen modelltípus példányait tartalmazó modellegyüttesek. Ez azt jelenti, hogy a mesterségesen generált adathalmaz által definiált osztályozási probléma megoldása során jelentős előnyt jelent a modellek diverzitása, vagy más szavakkal élve: ebben az esetben az eltérő megközelítést képviselő modellek predikciói jól kiegészítik egymást. Egy harmadik (és talán legáltalánosabb) mefogalmazása ennek a jelenségnek úgy hangzik, hogy a modellegyüttesben lévő egyes modellek jellemzően eltérő bemeneteken tévednek, így az általuk hozott egyszerű többségi döntés megbízhatóbb, mint az egyes modellek predikciója. Fontos megjegyezni azonban, hogy ez nem feltétlenül igaz minden osztályozási (vagy éppen regressziós) problémára, így a különböző megközelítések egyéni vizsgálata továbbra is fontos. Például előfordulhat, hogy a modellek többsége különösen rosszul teljesít az adathalmazon, így a modellegyüttes teljesítménye akár rosszabb is lehet néhány egyéni modell teljesítményénél."
      ],
      "metadata": {
        "id": "847fcmMd39I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Valós adathalmaz**\n",
        "\n",
        "A továbbiakban egy, az eddig használt, mesterségesen generált adathalmazhoz hasonló formátumú, ám valós adathalmazon fogjuk kipróbálni az eddig megismert módszereket.\n",
        "\n",
        "Ehhez először állítsuk alapállapotba a Colab környezetet, majd töltsük be a használandó adathalmazt:"
      ],
      "metadata": {
        "id": "rAfAzpPb4Ud9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "spambase_df = pd.read_csv(\"https://share.mit.bme.hu/index.php/s/wgc46gCHRb7bPdF/download/spambase.csv\")\n",
        "\n",
        "spambase_df"
      ],
      "metadata": {
        "id": "RE2zqLzZLAr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fenti adatbázis összesen 57 darab folytonos értékű változót, és egy bináris célváltozót tartalmaz. A bemeneti változók email-ekből kinyert leíró statisztikák, amelyek egyes szavak és karakterek relatív előfordulási gyakoriságát (százalékosan, a `word_freq_*` és `char_freq_*` kezdetű változók), illetve a csupa nagybetűs karaktersorozatok átlagos hosszát, a leghosszabb ilyen sorozat hosszát és a nagybetűs karakterek össz-mennyiségét adják meg. A célváltozó az adathalmaz utolsó oszlopában látható `Class`, amely megadja, hogy az eredeti email spam-nek számít-e (`Class=1`), vagy nem (`Class=0`).\n",
        "\n",
        "Ezt követően a tanításra való előkészítés végett bontsuk fel az adathalmazt tanító- és teszt részhalmazokra:"
      ],
      "metadata": {
        "id": "rV-b3OU0JNue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = spambase_df.drop(\"Class\", axis=1).values\n",
        "y = spambase_df[\"Class\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "JKHdr9ZRRKHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Feladatbeadás**\n",
        "\n",
        "A feladat egy spam-osztályozó készítése a fent ismertetett modellegyüttesek és a `spambase` adathalmaz felhasználásával.\n",
        "\n",
        "**Ezt a `spam_predictor.py` fájl alább látható szkeletonjának módosításával valósítsa meg!**\n",
        "\n",
        "A szkeletonban található `predict` függvényt módosítsa úgy, hogy az hozzon létre egy tetszőleges típusú modellegyüttest, tanítsa be a paraméterként kapott tanítóadaton (`X_train` és `y_train`) majd térjen vissza a szintén paraméterként kapott teszthalmazra (`X_test`) adott predikciókkal!\n",
        "\n",
        "> Tipp: A Moodle kiértékelő szerverén telepítve van az `sklearn` könyvtár, így érdemes az abban adott adott [modellegyüttes-módszereket](https://scikit-learn.org/stable/api/sklearn.ensemble.html), és [osztályozókat](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) használni a feladat megoldásához."
      ],
      "metadata": {
        "id": "ctuFvXq84XoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def predict(X_train, y_train, X_test):\n",
        "    ######################################################\n",
        "    # Create and fit an ensemble-based classifier here!\n",
        "    # ensemble_model = ...\n",
        "    ######################################################\n",
        "\n",
        "    base_model = DecisionTreeClassifier(max_depth=1)\n",
        "    ensemble_model = AdaBoostClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "    ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    return ensemble_model.predict(X_test)"
      ],
      "metadata": {
        "id": "vam0-xBNTiL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ezt követően ellenőrzésképpen vizsgáljuk meg, hogy a fent definiált `predict` függvény milyen pontosságot ér el a teszthalmazon:"
      ],
      "metadata": {
        "id": "r92bdywISLQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = predict(X_train, y_train, X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Ensemble Classifier Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "ENZbIRkESH5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A laborhoz tartozó Moodle-feladat akkor számít teljesítettnek, hogyha a `spambase` adatbázis egy különálló teszthalmazán legalább `93%`-os pontosságértéket sikerül elérni. A Moodle rendszerben használt teszthalmaz **nem azonos az itt látott teszthalmazzal**, de ugyanazokat a változókat tartalmazza.\n",
        "\n",
        "> **Fontos:** A feladat beadásánál figyeljen oda arra, hogy a beadott fájl neve `spam_predictor.py` legyen, és a fent látható szignatúrával azonos módon legyen benne definiálva a `predict` függvény!"
      ],
      "metadata": {
        "id": "GAq31x5tSpig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beadás:\n"
      ],
      "metadata": {
        "id": "NrpHGDW9WXQX"
      }
    }
  ]
}